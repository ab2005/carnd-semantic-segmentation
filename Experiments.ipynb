{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1 convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n",
      "Dense Output =\n",
      " [[[[-0.38790497  2.04511309]\n",
      "   [ 0.75163925 -3.96279335]]\n",
      "\n",
      "  [[ 0.63316619 -3.33817935]\n",
      "   [-0.33753824  1.77956951]]]]\n",
      "Conv 1x1 Output =\n",
      " [[[[-0.38790497  2.04511309]\n",
      "   [ 0.75163925 -3.96279335]]\n",
      "\n",
      "  [[ 0.63316619 -3.33817935]\n",
      "   [-0.33753824  1.77956951]]]]\n",
      "Same output? = True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# custom init with the seed set to 0 by default\n",
    "def custom_init(shape, dtype=tf.float32, partition_info=None, seed=0):\n",
    "    return tf.random_normal(shape, dtype=dtype, seed=seed)\n",
    "\n",
    "def conv_1x1(x, num_outputs, init):\n",
    "    return tf.layers.conv2d(x, num_outputs, 1, 1, kernel_initializer=init)\n",
    "\n",
    "x = tf.constant(np.random.randn(1, 2, 2, 1), dtype=tf.float32)\n",
    "\n",
    "# defines the number of output channels or kernels\n",
    "num_outputs = 2\n",
    "\n",
    "# `tf.layers.dense` flattens the input tensor if the rank > 2 and reshapes it back to the original rank\n",
    "# as the output.\n",
    "dense_out = tf.layers.dense(x, \n",
    "                            num_outputs, \n",
    "                            kernel_initializer=custom_init)\n",
    "conv_out = conv_1x1(x, \n",
    "                    num_outputs, \n",
    "                    init=custom_init)\n",
    "\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    a = sess.run(dense_out)\n",
    "    b = sess.run(conv_out)\n",
    "    print(\"Dense Output =\\n\", a)\n",
    "    print(\"Conv 1x1 Output =\\n\", b)\n",
    "\n",
    "    print(\"Same output? =\", np.allclose(a, b, atol=1.e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Transposed Convolution (deconvolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (1, 4, 4, 3)\n",
      "Output Shape: (1, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def upsample(x):\n",
    "    \"\"\"\n",
    "    Apply a two times upsample on x and return the result.\n",
    "    :x: 4-Rank Tensor\n",
    "    :return: TF Operation\n",
    "    \"\"\"\n",
    "    # TODO: Use `tf.layers.conv2d_transpose`\n",
    "    # The second argument 3 is the number of kernels/output channels.\n",
    "    num_kernels = 3\n",
    "    # The third argument is the kernel size, (2, 2). \n",
    "    # Note that the kernel size could also be (1, 1) and the output shape would be the same. \n",
    "    # However, if it were changed to (3, 3) note the shape would be (9, 9), at least with 'VALID' padding.\n",
    "    kernel_size = (2, 2)\n",
    "    # The fourth argument, the number of strides, is how we get from a height and width from (4, 4) to (8, 8). \n",
    "    # If this were a regular convolution the output height and width would be (2, 2).\n",
    "    num_strides = (2, 2)\n",
    "    return tf.layers.conv2d_transpose(x, num_kernels, kernel_size, num_strides)\n",
    "\n",
    "\n",
    "x = tf.constant(np.random.randn(1, 4, 4, 3), dtype=tf.float32)\n",
    "conv = upsample(x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    result = sess.run(conv)\n",
    "\n",
    "    print('Input Shape: {}'.format(x.get_shape()))\n",
    "    print('Output Shape: {}'.format(result.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN-8 - Encoder\n",
    "The FCN-8 architecture developed at Berkeley. In fact, many FCN models are derived from this FCN-8 implementation. \n",
    "\n",
    "The encoder for FCN-8 is the VGG16 model pretrained on ImageNet for classification. The fully-connected layers are replaced by 1-by-1 convolutions. Here’s an example of going from a fully-connected layer to a 1-by-1 convolution in TensorFlow:\n",
    "```python\n",
    "num_classes = 2\n",
    "output = tf.layers.dense(input, num_classes)\n",
    "```\n",
    "To:\n",
    "```python\n",
    "num_classes = 2\n",
    "output = tf.layers.conv2d(input, num_classes, 1, strides=(1,1))\n",
    "```\n",
    "The third argument, 1, is the kernel size, meaning this is a 1 by 1 convolution. Thus far, we’ve downsampled the input image and extracted features using the VGG16 encoder. We’ve also replaced the linear layers with 1 by 1 convolutional layers, preserving spatial information.\n",
    "\n",
    "But this is just the encoder portion of the network. Next comes the decoder.\n",
    ">Key observation is that fully connected layers in classification networks can be viewed as convolutions with kernels that cover their entire input regions. This is equivalent to evaluating the original classification network on overlapping input patches but is much more efficient because computation is shared over the overlapping regions of patches \n",
    "![](http://blog.qure.ai/assets/images/segmentation-review/FCN%20-%20illustration.png)\n",
    "\n",
    "[A 2017 Guide to Semantic Segmentation with Deep Learning](http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN-8 - Decoder\n",
    "\n",
    "To build the decoder portion of FCN-8, we’ll upsample the input to the original image size. The shape of the tensor after the final convolutional transpose layer will be 4-dimensional: \n",
    " - batch_size, \n",
    " - original_height, \n",
    " - original_width, \n",
    " - num_classes\n",
    " \n",
    "Let’s implement those transposed convolutions we discussed earlier as follows:\n",
    "```python\n",
    "output = tf.layers.conv2d_transpose(input, num_classes, 4, strides=(2, 2))\n",
    "```\n",
    "The transpose convolutional layers increase the height and width dimensions of the 4D input Tensor.\n",
    "\n",
    ">After convolutionalizing fully connected layers in a imagenet pretrained network like VGG, feature maps still need to be upsampled because of pooling operations in CNNs. Instead of using simple bilinear interpolation, deconvolutional layers **can learn the interpolation**. This layer is also known as upconvolution, full convolution, transposed convolution or fractionally-strided convolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Connections\n",
    "The final step is adding skip connections to the model. In order to do this we’ll combine the output of two layers. The first output is the output of the current layer. The second output is the output of a layer further back in the network, typically a pooling layer. In the following example we combine the result of the previous layer with the result of the 4th pooling layer through elementwise addition (tf.add).\n",
    "```\n",
    "# make sure the shapes are the same!\n",
    "input = tf.add(input, pool_4)\n",
    "```\n",
    "We can then follow this with another transposed convolution layer.\n",
    "```\n",
    "input = tf.layers.conv2d_transpose(input, num_classes, 4, strides=(2, 2))\n",
    "```\n",
    "We’ll repeat this once more with the third pooling layer output.\n",
    "```\n",
    "input = tf.add(input, pool_3)\n",
    "input = tf.layers.conv2d_transpose(input, num_classes, 16, strides=(8, 8))\n",
    "```\n",
    "\n",
    ">However, upsampling (even with deconvolutional layers) produces coarse segmentation maps because of loss of information during pooling. Therefore, shortcut/skip connections are introduced from higher resolution feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN-8 - Classification & Loss\n",
    "\n",
    "The final step is to define a loss. That way, we can approach training a FCN just like we would approach training a normal classification CNN.\n",
    "\n",
    "In the case of a FCN, the goal is to assign each pixel to the appropriate class. We already happen to know a great loss function for this setup, cross entropy loss! Remember the output tensor is 4D so we have to reshape it to 2D:\n",
    "\n",
    "```python\n",
    "...\n",
    "logits = tf.reshape(input, (-1, num_classes))\n",
    "```\n",
    "logits is now a 2D tensor where each row represents a pixel and each column a class. From here when just use standard \n",
    "cross entropy loss:\n",
    "```python\n",
    "cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "```\n",
    "That’s it, we now have an end-to-end model for semantic segmentation. Time to get training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    " - [carnd submission](https://github.com/dave-msk/CarND-Semantic-Segmentation-Submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
